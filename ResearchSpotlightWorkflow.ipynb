{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c226a23",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 2 - 5 minutes depending on the internet connection speed\n",
    "# If you can, switch to GPU for faster inference (Change runtime -> T4 GPU -> Connect)\n",
    "%%capture\n",
    "!pip install -r \"https://raw.githubusercontent.com/NikolasKapr/topostext-gazetteer-jsonld-format/refs/heads/main/collab/requirements.txt\"\n",
    "\n",
    "# Google Colab setup\n",
    "import os\n",
    "os.makedirs(\"Dataset\", exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/NikolasKapr/topostext-gazetteer-jsonld-format/refs/heads/main/example_subset_10.jsonl -P /content/Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd5438",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc4ad1",
   "metadata": {},
   "source": [
    "#### module initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import srsly\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup module paths\n",
    "m_ner_model_path = \"en_deberta_v3_base_ner_method\"\n",
    "a_ner_model_path = \"en_deberta_v3_base_ner_activity\"\n",
    "g_ner_model_path = \"en_deberta_v3_base_ner_goal\"\n",
    "\n",
    "# Setup input and output paths\n",
    "ee_input_path = \"./Dataset/example_subset_10.jsonl\"\n",
    "ee_output_path = \"./Dataset/example_subset_10_EE.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cfd1f",
   "metadata": {},
   "source": [
    "#### module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f09eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(model_path, in_data, entity):\n",
    "    print(\"NER for:\", entity)\n",
    "    ner_model = spacy.load(model_path)\n",
    "    annotated_data = []\n",
    "    for row in tqdm(in_data):\n",
    "        sent_nlp = ner_model(row[\"text\"])\n",
    "        ner_spans = [{\"start\": span.start_char, \"end\": span.end_char, \"token_start\":span.start, \"token_end\":span.end, \"mention\":row[\"text\"][span.start_char:span.end_char], \"label\": entity} for span in sent_nlp.ents]\n",
    "        if \"spans\" in row:\n",
    "            row[\"spans\"] += ner_spans\n",
    "        else:\n",
    "            row[\"spans\"] = ner_spans\n",
    "\n",
    "        row[\"_annotator_id\"] = \"NER\"\n",
    "        row[\"_session_id\"] = \"NER\"\n",
    "        annotated_data.append(row)\n",
    "\n",
    "    return(annotated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b3e32",
   "metadata": {},
   "source": [
    "#### module call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495080b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = srsly.read_jsonl(ee_input_path)\n",
    "\n",
    "sents_with_M = NER(m_ner_model_path, input_data, \"METHOD\")\n",
    "sents_with_A = NER(a_ner_model_path, sents_with_M, \"ACTIVITY\")\n",
    "sents_with_G = NER(g_ner_model_path, sents_with_A, \"GOAL\")\n",
    "\n",
    "srsly.write_jsonl(ee_output_path, sents_with_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a38285",
   "metadata": {},
   "source": [
    "#### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: Visualize the NER\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(sents_with_G[5][\"text\"])\n",
    "\n",
    "doc.spans[\"sc\"] = [\n",
    "    Span(doc, sents_with_G[5][\"spans\"][0][\"token_start\"], sents_with_G[5][\"spans\"][0][\"token_end\"], sents_with_G[5][\"spans\"][0][\"label\"]),\n",
    "    Span(doc, sents_with_G[5][\"spans\"][1][\"token_start\"], sents_with_G[5][\"spans\"][1][\"token_end\"], sents_with_G[5][\"spans\"][1][\"label\"]),\n",
    "    Span(doc, sents_with_G[5][\"spans\"][2][\"token_start\"], sents_with_G[5][\"spans\"][2][\"token_end\"], sents_with_G[5][\"spans\"][2][\"label\"])\n",
    "]\n",
    "\n",
    "colors = {\"METHOD\":\"cyan\", \"ACTIVITY\":\"orange\",\"GOAL\":\"lime\"}\n",
    "options = {\"colors\": colors}\n",
    "\n",
    "displacy.render(doc, jupyter=True, style=\"span\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6afe7",
   "metadata": {},
   "source": [
    "### Entity Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde31975",
   "metadata": {},
   "source": [
    "#### module initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fabd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from zshot import PipelineConfig, MentionsExtractor\n",
    "from zshot.linker import LinkerRegen\n",
    "from zshot.linker.linker_regen.utils import load_wikipedia_trie\n",
    "from zshot.utils.mappings import spans_to_wikipedia\n",
    "from zshot.utils.data_models import Span\n",
    "import srsly\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Silence most of the user warnings\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub.utils._auth\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.module\")\n",
    "# To use unverified ssl you can add this to your code\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# Load the Wikipedia trie\n",
    "wikipedia_trie = load_wikipedia_trie()\n",
    "\n",
    "# Setup input and output paths\n",
    "ed_input_path = \"./Dataset/example_subset_10_EE.jsonl\"\n",
    "ed_output_path = \"./Dataset/example_subset_10_EE_ED.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b2ac6",
   "metadata": {},
   "source": [
    "#### module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ed143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMentionExtractor(MentionsExtractor):\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "    def predict(self, docs, batch_size=None):\n",
    "        # Returns the character indexes of the mention spans for every text as spaCy Span object\n",
    "        return [[Span(start, end) for start, end in self.positions] for _ in docs]\n",
    "\n",
    "nlp_wikipedia = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to run Entity Disambiguation\n",
    "def genre_wikipedia(text, start, end):\n",
    "    nlp_config = PipelineConfig(\n",
    "        mentions_extractor=SimpleMentionExtractor([(start,end)]),\n",
    "        linker=LinkerRegen(trie=wikipedia_trie)\n",
    "    )\n",
    "    nlp_wikipedia.add_pipe(\"zshot\", config=nlp_config, last=True)\n",
    "    doc = nlp_wikipedia(text)\n",
    "    # Extract Wikipedia IDs from the spans after the entity linking, if no valid ID is found, return \"NIL\"\n",
    "    wikipedia_id = [i if i and \"=\" in i else \"NIL\" for i in spans_to_wikipedia(doc._.spans)]\n",
    "    nlp_wikipedia.remove_pipe(\"zshot\")\n",
    "    return wikipedia_id[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7b241",
   "metadata": {},
   "source": [
    "#### module call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89000a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 5 minutes\n",
    "data = list(srsly.read_jsonl(ed_input_path))\n",
    "for item in tqdm(data):\n",
    "    text = item.get(\"text\")\n",
    "    for span in item.get(\"spans\", []):\n",
    "        if span.get(\"label\") == \"METHOD\":\n",
    "            span[\"wikipedia_url\"] = genre_wikipedia(text, span[\"start\"], span[\"end\"])\n",
    "\n",
    "srsly.write_jsonl(ed_output_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8a567",
   "metadata": {},
   "source": [
    "### Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891e93f",
   "metadata": {},
   "source": [
    "#### module initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from tqdm import tqdm\n",
    "from information_linking_queries.information_linking_orcid import information_linking_orcid\n",
    "from information_linking_queries.information_linking_apis import information_linking\n",
    "\n",
    "# Setup input and output paths\n",
    "el_input_path = \"./Dataset/example_subset_10_EE_ED.jsonl\"\n",
    "el_output_path = \"./Dataset/example_subset_10_EE_ED_EL.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189bc8",
   "metadata": {},
   "source": [
    "#### module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f114fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_author_info(l_row):\n",
    "    author_list = []\n",
    "    for a in l_row.get(\"meta\").get(\"creator\", []):\n",
    "        # Retrieve the first and last name of the author\n",
    "        # Check if the first word is bigger than two characters\n",
    "        f_name = a.split()[0].strip()\n",
    "        l_name = a.split()[-1].strip()\n",
    "\n",
    "        try:\n",
    "            orcid_info = information_linking_orcid(f_name, l_name)\n",
    "            author_list.append({'full_name':a, 'given_name': orcid_info['given-names'], 'family_name':orcid_info['family-names'], 'orcid': orcid_info['orcid-id'], 'affiliations':orcid_info['institution-name'], 'email':orcid_info['email']})\n",
    "        except:\n",
    "            author_list.append({'full_name':a, 'given_name': f_name.capitalize(), 'family_name':l_name.capitalize(), 'orcid': 'None', 'affiliations':'None', 'email':'None'})\n",
    "    l_row['meta']['creator'] = author_list\n",
    "\n",
    "    return(l_row)\n",
    "\n",
    "def link_method_info(l_row):\n",
    "    for label in l_row.get(\"spans\", []):\n",
    "        if label.get(\"label\") == \"METHOD\":\n",
    "            method_info = information_linking(wikipedia_url=label[\"wikipedia_url\"])\n",
    "\n",
    "            label[\"description\"] = method_info[\"description\"]\n",
    "            label[\"proper_name\"] = method_info[\"label\"]\n",
    "            label[\"aliases\"] = method_info[\"aliases\"]\n",
    "            label[\"wikidata_url\"] = method_info[\"wikidata\"]\n",
    "            label[\"dbpedia_url\"] = method_info[\"dbpedia\"]\n",
    "\n",
    "    return(l_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9420d2f9",
   "metadata": {},
   "source": [
    "#### module call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ae260",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = list(srsly.read_jsonl(el_input_path))\n",
    "linked_data = []\n",
    "\n",
    "for row in tqdm(in_data):\n",
    "    row = link_author_info(row)\n",
    "    row = link_method_info(row)\n",
    "    linked_data.append(row)\n",
    "\n",
    "srsly.write_jsonl(el_output_path, linked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e280f",
   "metadata": {},
   "source": [
    "#### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Entity Disambiguation for methods\n",
    "from spacy import displacy\n",
    "\n",
    "example = [{\"text\": linked_data[6][\"text\"],\n",
    "            \"ents\": [{\"start\": linked_data[6][\"spans\"][0][\"start\"], \"end\": linked_data[6][\"spans\"][0][\"end\"], \"label\": linked_data[6][\"spans\"][0][\"label\"], \"kb_id\": linked_data[6][\"spans\"][0][\"wikidata_url\"].split(\"/\")[-1], \"kb_url\": linked_data[6][\"spans\"][0][\"wikidata_url\"]},\n",
    "                     {\"start\": linked_data[6][\"spans\"][1][\"start\"], \"end\": linked_data[6][\"spans\"][1][\"end\"], \"label\": linked_data[6][\"spans\"][1][\"label\"], \"kb_id\": linked_data[6][\"spans\"][0][\"wikidata_url\"].split(\"/\")[-1], \"kb_url\": linked_data[6][\"spans\"][0][\"wikidata_url\"]},\n",
    "                     {\"start\": linked_data[6][\"spans\"][2][\"start\"], \"end\": linked_data[6][\"spans\"][2][\"end\"], \"label\": linked_data[6][\"spans\"][2][\"label\"], \"kb_id\": linked_data[6][\"spans\"][0][\"wikidata_url\"].split(\"/\")[-1], \"kb_url\": linked_data[6][\"spans\"][0][\"wikidata_url\"]}]}]\n",
    "\n",
    "options = {\"colors\": {\"METHOD\":\"cyan\"}}\n",
    "\n",
    "displacy.render(example, style=\"ent\", jupyter=True, manual=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bea5a2",
   "metadata": {},
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324f02b",
   "metadata": {},
   "source": [
    "#### module initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input and output paths\n",
    "re_input_path = \"./Dataset/example_subset_10_EE_ED_EL.jsonl\"\n",
    "re_output_path = \"./Dataset/example_subset_10_EE_ED_EL_RE.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499f973",
   "metadata": {},
   "source": [
    "#### module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(a_start, a_end, m_start, m_end):\n",
    "    return max(a_start, m_start) < min(a_end, m_end)\n",
    "\n",
    "def relation_extraction_employs(spans, text):\n",
    "    activity_list = []\n",
    "    methods_list = []\n",
    "    for span in spans:\n",
    "      if span.get(\"label\") == \"ACTIVITY\":\n",
    "        activity_list.append((span.get(\"start\"), span.get(\"end\"), span.get(\"label\"), text))\n",
    "      if span.get(\"label\") == \"METHOD\":\n",
    "        methods_list.append((span.get(\"start\"), span.get(\"end\"), span.get(\"label\")))\n",
    "\n",
    "    relation = []\n",
    "    for domain in activity_list:\n",
    "      activity_begin_num = domain[0]\n",
    "      activity_end_num = domain[1]\n",
    "      for range in methods_list:\n",
    "        method_begin_num = range[0]\n",
    "        method_end_num = range[1]\n",
    "        if is_overlapping(activity_begin_num, activity_end_num, method_begin_num, method_end_num):\n",
    "          relation.append({\"domain\":{\"start\":activity_begin_num, \"end\":activity_end_num, \"span\":domain[3][activity_begin_num:activity_end_num],\"label\":domain[2]},\n",
    "                           \"range\":{\"start\":method_begin_num, \"end\":method_end_num, \"span\":domain[3][method_begin_num:method_end_num], \"label\":range[2]},\n",
    "                           \"label\":\"EMPLOYS\"})\n",
    "    return relation\n",
    "\n",
    "def relation_extraction_hasObjective(spans, text):\n",
    "    activity_list = []\n",
    "    goal_list = []\n",
    "    for span in spans:\n",
    "      if span.get(\"label\") == \"ACTIVITY\":\n",
    "        activity_list.append((span.get(\"start\"), span.get(\"end\"), span.get(\"label\"), text))\n",
    "      if span.get(\"label\") == \"GOAL\":\n",
    "        goal_list.append((span.get(\"start\"), span.get(\"end\"), span.get(\"label\")))\n",
    "\n",
    "    relation = []\n",
    "    for domain in activity_list:\n",
    "      activity_begin_num = domain[0]\n",
    "      activity_end_num = domain[1]\n",
    "      for range in goal_list:\n",
    "        goal_begin_num = range[0]\n",
    "        goal_end_num = range[1]\n",
    "        relation.append({\"domain\":{\"start\":activity_begin_num, \"end\":activity_end_num, \"span\":domain[3][activity_begin_num:activity_end_num],\"label\":domain[2]},\n",
    "                        \"range\":{\"start\":goal_begin_num, \"end\":goal_end_num, \"span\":domain[3][goal_begin_num:goal_end_num], \"label\":range[2]},\n",
    "                        \"label\":\"HAS_OBJECTIVE\"})\n",
    "    return relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edd5fd",
   "metadata": {},
   "source": [
    "#### module call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(srsly.read_jsonl(re_input_path))\n",
    "for i in tqdm(data):\n",
    "  text = i.get(\"text\")\n",
    "  spans = i.get(\"spans\")\n",
    "  checker = []\n",
    "  relations_employs = []\n",
    "  relations_hasObjective = []\n",
    "  for span in spans:\n",
    "    checker.append(span.get(\"label\"))\n",
    "  if \"ACTIVITY\" in checker and \"METHOD\" in checker:\n",
    "    relations_employs = relation_extraction_employs(spans, text)\n",
    "  if \"ACTIVITY\" in checker and \"GOAL\" in checker:\n",
    "    relations_hasObjective = relation_extraction_hasObjective(spans, text)\n",
    "  i[\"relations\"] = relations_employs + relations_hasObjective\n",
    "\n",
    "srsly.write_jsonl(re_output_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd0bf5",
   "metadata": {},
   "source": [
    "### RDF Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f710b1",
   "metadata": {},
   "source": [
    "#### module initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c688a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "from rdflib import Graph, Namespace, RDF, URIRef, RDFS, Literal, OWL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83d0ed",
   "metadata": {},
   "source": [
    "#### module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775de705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_triples(jstor_row, article_uri, schema_ns, instances_ns, GraphObject):\n",
    "    query = \"\"\"\n",
    "                    PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                    SELECT DISTINCT ?m_name\n",
    "                    WHERE {\n",
    "                        ?m_name rdf:type so:Sentence.\n",
    "                    }\"\"\"\n",
    "\n",
    "    s_list = [i[0] for i in GraphObject.query(query)]\n",
    "\n",
    "    #print('article id:', re.sub('.*/','',jstor_row['meta']['id']))\n",
    "    sent_uri = str(instances_ns)+'Sentence/'+str(row['meta']['sent_no'])\n",
    "    if URIRef(sent_uri) not in s_list:\n",
    "        GraphObject.add((URIRef(sent_uri), RDF.type, schema_ns.Sentence))\n",
    "        GraphObject.add((URIRef(sent_uri), schema_ns.sentence_text, Literal(jstor_row['text']) ))\n",
    "        GraphObject.add((URIRef(sent_uri), schema_ns.is_part_of, URIRef(article_uri) ))\n",
    "\n",
    "    return(GraphObject, sent_uri)\n",
    "\n",
    "\n",
    "def create_article_triples(jstor_row, schema_ns, instances_ns, GraphObject):\n",
    "    query = \"\"\"\n",
    "                    PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                    SELECT DISTINCT ?m_name\n",
    "                    WHERE {\n",
    "                        ?m_name rdf:type so:Article.\n",
    "                    }\"\"\"\n",
    "\n",
    "    a_list = [i[0] for i in GraphObject.query(query)]\n",
    "\n",
    "    article_uri = str(instances_ns)+'Article/'+re.sub('.*/','',row['meta']['id'])\n",
    "\n",
    "    if URIRef(article_uri) not in a_list:\n",
    "        GraphObject.add((URIRef(article_uri), RDF.type, schema_ns.Article))\n",
    "        GraphObject.add((URIRef(article_uri), schema_ns.title, Literal(jstor_row['meta']['title']) ))\n",
    "        if 'url' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.article_URL, Literal(jstor_row['meta']['url']) ))\n",
    "\n",
    "        local_doi = None\n",
    "        for id_dict in jstor_row['meta'].get('identifier', []):\n",
    "            if id_dict.get('name') == 'local_doi':\n",
    "                local_doi = id_dict.get('value')\n",
    "                break\n",
    "        if local_doi:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.article_DOI, Literal(local_doi)))\n",
    "\n",
    "        if 'datePublished' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.publication_date, Literal(jstor_row['meta']['datePublished']) ))\n",
    "        if 'publicationYear' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.publication_year, Literal(jstor_row['meta']['publicationYear']) ))\n",
    "        if 'issueNumber' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.issue_number, Literal(jstor_row['meta']['issueNumber']) ))\n",
    "        if 'publisher' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.publisher, Literal(jstor_row['meta']['publisher']) ))\n",
    "        if 'pageCount' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.page_count, Literal(jstor_row['meta']['pageCount']) ))\n",
    "        if 'docType' in jstor_row['meta']:\n",
    "            GraphObject.add((URIRef(article_uri), schema_ns.doctype, Literal(jstor_row['meta']['docType']) ))\n",
    "\n",
    "\n",
    "    return(GraphObject, article_uri)\n",
    "\n",
    "\n",
    "def create_aggregation_triples(jstor_row, article_uri, schema_ns, instances_ns, GraphObject):\n",
    "    query = \"\"\"\n",
    "                PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                SELECT DISTINCT ?m_name\n",
    "                WHERE {\n",
    "                    ?m_name rdf:type so:Aggregation.\n",
    "                }\"\"\"\n",
    "\n",
    "    a_list = [i[0] for i in GraphObject.query(query)]\n",
    "\n",
    "    aggregation_uri = str(instances_ns)+'Aggregation/'+re.sub(' ','_',jstor_row['meta']['isPartOf'])\n",
    "\n",
    "    if URIRef(aggregation_uri) not in a_list:\n",
    "        GraphObject.add((URIRef(aggregation_uri), RDF.type, schema_ns.Aggregation))\n",
    "        GraphObject.add((URIRef(aggregation_uri), schema_ns.aggregation_name, Literal(jstor_row['meta']['isPartOf']) ))\n",
    "\n",
    "    GraphObject.add((URIRef(article_uri), schema_ns.is_member_of, URIRef(aggregation_uri) ))\n",
    "\n",
    "    return(GraphObject)\n",
    "\n",
    "\n",
    "def create_topic_triples(jstor_row, article_uri, schema_ns, instances_ns, GraphObject):\n",
    "    query = \"\"\"\n",
    "                PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                SELECT DISTINCT ?m_name\n",
    "                WHERE {\n",
    "                    ?m_name rdf:type so:Topic.\n",
    "                }\"\"\"\n",
    "\n",
    "    a_list = [i[0] for i in GraphObject.query(query)]\n",
    "\n",
    "    for t in jstor_row['meta']['topics']:\n",
    "        topic_uri = str(instances_ns)+'Topic/'+re.sub(' ','_',t)\n",
    "        if URIRef(topic_uri) not in a_list:\n",
    "            GraphObject.add((URIRef(topic_uri), RDF.type, schema_ns.Topic))\n",
    "            GraphObject.add((URIRef(topic_uri), schema_ns.topic_name, Literal(t) ))\n",
    "\n",
    "        GraphObject.add((URIRef(topic_uri), schema_ns.is_topic_of, URIRef(article_uri) ))\n",
    "\n",
    "    return(GraphObject)\n",
    "\n",
    "\n",
    "def create_author_organization_triples(jstor_row, article_uri, schema_ns, instances_ns, GraphObject):\n",
    "    query = \"\"\"\n",
    "                    PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                    SELECT DISTINCT ?m_name\n",
    "                    WHERE {\n",
    "                        ?m_name rdf:type so:Person.\n",
    "                    }\"\"\"\n",
    "\n",
    "    p_list = [i[0] for i in GraphObject.query(query)]\n",
    "    query = \"\"\"\n",
    "                    PREFIX so: <https://scholarlyontology.aueb.gr/resources/so_schema/so_jstor_1.0#>\n",
    "                    SELECT DISTINCT ?m_name\n",
    "                    WHERE {\n",
    "                        ?m_name rdf:type so:Organization.\n",
    "                    }\"\"\"\n",
    "\n",
    "    o_list = [i[0] for i in GraphObject.query(query)]\n",
    "\n",
    "    authors = []\n",
    "    for crt in jstor_row['meta']['creator']:\n",
    "        if crt['orcid'] != 'None':\n",
    "            author_uri = str(instances_ns)+'Person/'+str(crt['orcid'])\n",
    "            if URIRef(author_uri) not in p_list:\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.orcid, Literal(crt['orcid']) ))\n",
    "                GraphObject.add((URIRef(author_uri), RDF.type, schema_ns.Person))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.full_name, Literal(crt['full_name']) ))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.family_name, Literal(crt['family_name']) ))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.given_name, Literal(crt['given_name']) ))\n",
    "\n",
    "                for org in crt['affiliations']:\n",
    "                    org_uri = str(instances_ns)+'Organization/'+re.sub(' ', '_', org)\n",
    "                    if URIRef(org_uri) not in o_list:\n",
    "                        GraphObject.add((URIRef(org_uri), RDF.type, schema_ns.Organization))\n",
    "                        GraphObject.add((URIRef(org_uri), schema_ns.organization_name, Literal(org) ))\n",
    "                    GraphObject.add((URIRef(author_uri), schema_ns.is_affiliated_to, URIRef(org_uri) ))\n",
    "\n",
    "        elif crt['orcid'] == 'None':\n",
    "            author_uri = str(instances_ns)+'Person/' + re.sub(' ','_',crt['full_name'])\n",
    "            if URIRef(author_uri) not in p_list:\n",
    "                GraphObject.add((URIRef(author_uri), RDF.type, schema_ns.Person))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.full_name, Literal(crt['full_name']) ))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.family_name, Literal(crt['family_name']) ))\n",
    "                GraphObject.add((URIRef(author_uri), schema_ns.given_name, Literal(crt['given_name']) ))\n",
    "\n",
    "        GraphObject.add((URIRef(author_uri), schema_ns.is_author_of, URIRef(article_uri) ))\n",
    "\n",
    "        authors.append(author_uri)\n",
    "\n",
    "    return(GraphObject, authors)\n",
    "\n",
    "\n",
    "def create_activity_triples(row, act_span, author_uris, sent_uri, schema_ns, instances_ns, GraphObject):\n",
    "    act_uri = str(instances_ns)+'Activity/'+str(row['meta']['sent_no'])+ '_'+ str(act_span['start'])+'_'+str(act_span['end'])\n",
    "    GraphObject.add((URIRef(act_uri), RDF.type, schema_ns.Activity))\n",
    "    GraphObject.add((URIRef(act_uri), schema_ns.textual_span, Literal(row['text'][act_span['start']:act_span['end']]) ))\n",
    "    GraphObject.add((URIRef(act_uri), schema_ns.has_sentence_context, URIRef(sent_uri) ))\n",
    "    GraphObject.add((URIRef(act_uri), schema_ns.begin_index, Literal(act_span['start']) ))\n",
    "    GraphObject.add((URIRef(act_uri), schema_ns.end_index, Literal(act_span['end']) ))\n",
    "    #GraphObject.add((URIRef(act_uri), schema_ns.has_article_context, URIRef(article_uri) ))\n",
    "    for a_uri in author_uris:\n",
    "        GraphObject.add((URIRef(a_uri), schema_ns.participates_in, URIRef(act_uri) ))\n",
    "\n",
    "    return(GraphObject)\n",
    "\n",
    "def create_goal_triples(row, g_span, author_uris, sent_uri, schema_ns, instances_ns, GraphObject):\n",
    "    goal_uri = str(instances_ns)+'Goal/'+str(row['meta']['sent_no'])+ '_'+ str(g_span['start'])+'_'+str(g_span['end'])\n",
    "    GraphObject.add((URIRef(goal_uri), RDF.type, schema_ns.Goal))\n",
    "    GraphObject.add((URIRef(goal_uri), schema_ns.textual_span, Literal(row['text'][g_span['start']:g_span['end']]) ))\n",
    "    GraphObject.add((URIRef(goal_uri), schema_ns.has_sentence_context, URIRef(sent_uri) ))\n",
    "    GraphObject.add((URIRef(goal_uri), schema_ns.begin_index, Literal(g_span['start']) ))\n",
    "    GraphObject.add((URIRef(goal_uri), schema_ns.end_index, Literal(g_span['end']) ))\n",
    "    #GraphObject.add((URIRef(goal_uri), schema_ns.has_article_context, URIRef(article_uri) ))\n",
    "    for a_uri in author_uris:\n",
    "        GraphObject.add((URIRef(a_uri), schema_ns.has_goal, URIRef(goal_uri) ))\n",
    "\n",
    "    return(GraphObject)\n",
    "\n",
    "\n",
    "def create_method_triples(m_span, sent_uri, sent_no, schema_ns, instances_ns, GraphObject, sentence_text):\n",
    "    \"\"\"\n",
    "    Create a unique Method node for each occurrence in a sentence.\n",
    "    \"\"\"\n",
    "    # Make URI unique per sentence & span\n",
    "    if 'qid' in m_span and m_span['qid'] != 'None':\n",
    "        m_uri = str(instances_ns) + 'Method/' + str(m_span['qid']) +\"_\"+ str(sent_no)+\"_\"+ str(m_span[\"start\"]) + \"_\"+ str(m_span[\"end\"])\n",
    "    else:\n",
    "        m_uri = str(instances_ns) + 'Method/' + re.sub(' ', '_', m_span['proper_name']) +\"_\"+ str(sent_no) +\"_\"+ str(m_span[\"start\"]) + \"_\"+ str(m_span[\"end\"])\n",
    "\n",
    "    # Create the node\n",
    "    GraphObject.add((URIRef(m_uri), RDF.type, schema_ns.Method))\n",
    "    GraphObject.add((URIRef(m_uri), schema_ns.method_name, Literal(str(m_span['proper_name']))))\n",
    "    GraphObject.add((URIRef(m_uri), schema_ns.textual_span, Literal(sentence_text[m_span['start']:m_span['end']])))\n",
    "    GraphObject.add((URIRef(m_uri), schema_ns.has_sentence_context, URIRef(sent_uri)))\n",
    "    #GraphObject.add((URIRef(m_uri), schema_ns.has_article_context, URIRef(article_uri)))\n",
    "\n",
    "    # Optional metadata\n",
    "    if 'wikidata_url' in m_span and m_span['wikidata_url'] not in [None, 'None']:\n",
    "        GraphObject.add((URIRef(m_uri), schema_ns.wikidata_url, Literal(str(m_span['wikidata_url']))))\n",
    "    if 'description' in m_span and m_span['description'] not in [None, 'None']:\n",
    "        GraphObject.add((URIRef(m_uri), schema_ns.description, Literal(str(m_span['description']))))\n",
    "    if 'aliases' in m_span and m_span['aliases'] not in [None, []]:\n",
    "        GraphObject.add((URIRef(m_uri), schema_ns.aliases, Literal(str(m_span['aliases']))))\n",
    "    if 'wikipedia_url' in m_span and m_span['wikipedia_url'] not in [None, 'None']:\n",
    "        GraphObject.add((URIRef(m_uri), schema_ns.wikipedia_url, Literal(str(m_span['wikipedia_url']))))\n",
    "    if 'qid' in m_span and m_span['qid'] != 'None':\n",
    "        GraphObject.add((URIRef(m_uri), schema_ns.qid, Literal(str(m_span['qid']))))\n",
    "\n",
    "    return m_uri, GraphObject\n",
    "\n",
    "\n",
    "def spans_overlap(start1, end1, start2, end2):\n",
    "    return not (end1 < start2 or end2 < start1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f197a",
   "metadata": {},
   "source": [
    "#### schema decleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ns = Namespace(\"https://scholarlyontology.aueb.gr/resources/so_schema/so_MLE#\")\n",
    "instances_ns = Namespace(\"https://scholarlyontology.aueb.gr/resources/so_instances/so_MLE#\")\n",
    "nif_ns = Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"so\", schema_ns)\n",
    "g.bind(\"inst\", instances_ns)\n",
    "g.bind(\"nif\", nif_ns)\n",
    "\n",
    "\n",
    "# Classes\n",
    "def declare_class(cls, parent=None):\n",
    "    g.add((cls, RDF.type, RDFS.Class))\n",
    "    if parent:\n",
    "        g.add((cls, RDFS.subClassOf, parent))\n",
    "\n",
    "# Higher level\n",
    "declare_class(schema_ns.SO_Entity)\n",
    "declare_class(schema_ns.Object, schema_ns.SO_Entity)\n",
    "declare_class(schema_ns.ConceptualObject, schema_ns.Object)\n",
    "declare_class(schema_ns.InformationResource, schema_ns.ConceptualObject)\n",
    "declare_class(schema_ns.Assertion, schema_ns.ConceptualObject)\n",
    "declare_class(schema_ns.Event, schema_ns.SO_Entity)\n",
    "declare_class(schema_ns.Actor, schema_ns.SO_Entity)\n",
    "declare_class(schema_ns.Group, schema_ns.Actor)\n",
    "declare_class(schema_ns.ContentItem, schema_ns.InformationResource)\n",
    "\n",
    "# With instances\n",
    "declare_class(schema_ns.Activity, schema_ns.Event)\n",
    "declare_class(schema_ns.Method, schema_ns.ConceptualObject)\n",
    "declare_class(schema_ns.Topic, schema_ns.ConceptualObject)\n",
    "declare_class(schema_ns.Goal, schema_ns.Assertion)\n",
    "declare_class(schema_ns.Person, schema_ns.Actor)\n",
    "declare_class(schema_ns.Organization, schema_ns.Group)\n",
    "declare_class(schema_ns.Article, schema_ns.ContentItem)\n",
    "declare_class(schema_ns.Sentence, schema_ns.ContentItem)\n",
    "declare_class(schema_ns.Aggregation, schema_ns.InformationResource)\n",
    "\n",
    "\n",
    "# Object properties\n",
    "def declare_object_property(p, domain, range):\n",
    "    g.add((p, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((p, RDFS.domain, domain))\n",
    "    g.add((p, RDFS.range, range))\n",
    "\n",
    "declare_object_property(schema_ns.employs, schema_ns.Activity, schema_ns.Method)\n",
    "declare_object_property(schema_ns.participates_in, schema_ns.Person, schema_ns.Activity)\n",
    "declare_object_property(schema_ns.has_goal, schema_ns.Person, schema_ns.Goal)\n",
    "declare_object_property(schema_ns.has_objective, schema_ns.Activity, schema_ns.Goal)\n",
    "declare_object_property(schema_ns.has_sentence_context, nif_ns.String, nif_ns.Context)\n",
    "declare_object_property(schema_ns.is_member_of, schema_ns.Article, schema_ns.Aggregation)\n",
    "declare_object_property(schema_ns.is_topic_of, schema_ns.Topic, schema_ns.Article)\n",
    "declare_object_property(schema_ns.uses_method, schema_ns.Person, schema_ns.Method)\n",
    "declare_object_property(schema_ns.is_part_of, schema_ns.Sentence, schema_ns.Article)\n",
    "declare_object_property(schema_ns.is_author_of, schema_ns.Person, schema_ns.Article)\n",
    "declare_object_property(schema_ns.is_affiliated_to, schema_ns.Person, schema_ns.Organization)\n",
    "\n",
    "\n",
    "# Datatype properties\n",
    "def declare_datatype_property(p, domain):\n",
    "    g.add((p, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((p, RDFS.domain, domain))\n",
    "\n",
    "declare_datatype_property(schema_ns.full_name, schema_ns.Person)\n",
    "declare_datatype_property(schema_ns.given_name, schema_ns.Person)\n",
    "declare_datatype_property(schema_ns.family_name, schema_ns.Person)\n",
    "declare_datatype_property(schema_ns.orcid, schema_ns.Person)\n",
    "\n",
    "declare_datatype_property(schema_ns.begin_index, nif_ns.String)\n",
    "declare_datatype_property(schema_ns.end_index, nif_ns.String)\n",
    "\n",
    "declare_datatype_property(schema_ns.publication_year, schema_ns.Article)\n",
    "declare_datatype_property(schema_ns.title, schema_ns.Article)\n",
    "declare_datatype_property(schema_ns.article_DOI, schema_ns.Article)\n",
    "\n",
    "declare_datatype_property(schema_ns.organization_name, schema_ns.Organization)\n",
    "declare_datatype_property(schema_ns.aggregation_name, schema_ns.Aggregation)\n",
    "\n",
    "declare_datatype_property(schema_ns.sentence_text, schema_ns.Sentence)\n",
    "declare_datatype_property(schema_ns.topic_name, schema_ns.Topic)\n",
    "\n",
    "declare_datatype_property(schema_ns.wikidata_url, schema_ns.Method)\n",
    "declare_datatype_property(schema_ns.qid, schema_ns.Method)\n",
    "\n",
    "\n",
    "# Save schema\n",
    "g.serialize(\"./Dataset/so_MLE_schema.rdf\", format=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b084484",
   "metadata": {},
   "source": [
    "#### module call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema\n",
    "g = Graph()\n",
    "g.parse(\"./Dataset/so_MLE_schema.rdf\", format=\"xml\")\n",
    "g.bind(\"so\", schema_ns)\n",
    "g.bind(\"inst\", instances_ns)\n",
    "g.bind(\"nif\", nif_ns)\n",
    "\n",
    "\n",
    "# Module call\n",
    "input_path = \"./Dataset/example_subset_10_EE_ED_EL_RE.jsonl\"\n",
    "\n",
    "in_data = srsly.read_jsonl(input_path)\n",
    "\n",
    "article_id = ''\n",
    "rows = []\n",
    "\n",
    "\n",
    "for row in tqdm(in_data):\n",
    "    if row[\"answer\"] == \"accept\" or row[\"answer\"] == \"ignore\":\n",
    "        activity_uris = []\n",
    "        goal_uris = []\n",
    "        method_uris = []\n",
    "        g, article_uri = create_article_triples(row, schema_ns, instances_ns, g)\n",
    "        g, sent_uri = create_sentence_triples(row, article_uri, schema_ns, instances_ns, g)\n",
    "        try:\n",
    "            g = create_aggregation_triples(row, article_uri, schema_ns, instances_ns, g)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            g = create_topic_triples(row, article_uri, schema_ns, instances_ns, g)\n",
    "        except:\n",
    "            pass\n",
    "        g, author_uris = create_author_organization_triples(row, article_uri, schema_ns, instances_ns, g)\n",
    "\n",
    "        if len(row['spans']) >0:\n",
    "            for span in row['spans']:\n",
    "                if span['label'] == 'ACTIVITY':\n",
    "                    g = create_activity_triples(row, span, author_uris, sent_uri, schema_ns, instances_ns, g)\n",
    "                    activity_uri = str(instances_ns)+'Activity/'+str(row['meta']['sent_no'])+ '_'+ str(span['start'])+'_'+str(span['end'])\n",
    "                    activity_uris.append({'uri': activity_uri, 'span': span})\n",
    "                elif span['label'] == 'METHOD':\n",
    "                    method_uri, g = create_method_triples(span, sent_uri, row['meta']['sent_no'], schema_ns, instances_ns, g, row['text'])\n",
    "                    method_uris.append({'uri': method_uri, 'span': span})\n",
    "                elif span['label'] == 'GOAL':\n",
    "                    g = create_goal_triples(row, span, author_uris, sent_uri, schema_ns, instances_ns, g)\n",
    "                    goal_uri = str(instances_ns)+'Goal/'+str(row['meta']['sent_no'])+ '_'+ str(span['start'])+'_'+str(span['end'])\n",
    "                    goal_uris.append({'uri': goal_uri, 'span': span})\n",
    "\n",
    "            for act in activity_uris:\n",
    "                act_start, act_end = act['span']['start'], act['span']['end']\n",
    "\n",
    "                for meth in method_uris:\n",
    "                    m_start, m_end = meth['span']['start'], meth['span']['end']\n",
    "                    if spans_overlap(int(act_start), int(act_end), int(m_start), int(m_end)):\n",
    "                        g.add((URIRef(act['uri']), schema_ns.employs, URIRef(meth['uri'])))\n",
    "\n",
    "            for act in activity_uris:\n",
    "                act_uri = URIRef(act['uri'])\n",
    "                for goal in goal_uris:\n",
    "                    goal_uri = URIRef(goal['uri'])\n",
    "                    # If you want to restrict to overlapping spans, uncomment the if:\n",
    "                    # g_start, g_end = goal['span']['start'], goal['span']['end']\n",
    "                    # if spans_overlap(act['span']['start'], act['span']['end'], g_start, g_end):\n",
    "                    g.add((act_uri, schema_ns.has_objective, goal_uri))\n",
    "\n",
    "            for act in activity_uris:\n",
    "                act_uri = URIRef(act['uri'])\n",
    "                for meth in method_uris:\n",
    "                    meth_uri = URIRef(meth['uri'])\n",
    "                    if (act_uri, schema_ns.employs, meth_uri) in g:\n",
    "                        for person_uri in [p for p in author_uris if (URIRef(p), schema_ns.participates_in, act_uri) in g]:\n",
    "                            g.add((URIRef(person_uri), schema_ns.uses_method, meth_uri))\n",
    "\n",
    "# Save RDF file\n",
    "output_path = \"./Dataset/MLE_RDF.rdf\"\n",
    "g.serialize(destination=output_path, format='xml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
